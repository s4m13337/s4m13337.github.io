<!DOCTYPE html>
<html>
  <head>
    <title>Chapter 7: Ensemble Learning - s4m13337</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/style.css">
  </head>
  <body>
  	<div class="container">
  		<h1 class="header">s4m13337's website</h1>

      <!-- Navigation -->
      
      
      <nav>
        <ul>
        
          <li><a href="/index.html">üè† Home</a></li>
        
          <li><a href="/about.html">‚ÑπÔ∏è About</a></li>
        
          <li><a href="/blog/index.html">üìù Blog</a></li>
        
          <li><a href="/contact.html">‚úâÔ∏è Contact</a></li>
        
        </ul>
      </nav>

      <!-- Content area -->
      <hr>
      <div class="content">
	<h1 style="text-align: center;">Chapter 7: Ensemble Learning</h1>
	<h5 style="text-align: center;"><em>[2022-05-19]</em></h5>
	<p><h2>Questions</h2>
<h3>What is the idea behind ensemble learning?</h3>
<ul>
<li>In ensemble learning, a group of predictors are trained.</li>
<li>The outputs from all the predictors are aggregated in order to obtain the final output.</li>
</ul>
<h3>What can improve the ensemble‚Äôs accuracy?</h3>
<ul>
<li>An ensemble's accuracy can be increased by increasing the number of predictors and ensuring that each individual predictors are sufficiently diverse.</li>
<li>Ensemble methods work best when the predictors are as much independent from each other as possible.</li>
<li>To get diverse predictors, each of them can be trained either with different set of algorithms or with different subsets of the training set. This ensurse that they will make different types of errors, thus increasing the ensemble's accuracy.</li>
</ul>
<h3>What is the difference between bagging and pasting?</h3>
<ul>
<li>Both bagging and pasting are sampling methods for creating training subsets in ensemble learning.</li>
<li>When sampling is performed with replacement, it is called <em>bagging</em> (Bootstrap aggregation).</li>
<li>When sampling is done without replacement, it is called <em>pasting</em>.</li>
</ul>
<h3>How does out of bag evaluation work?</h3>
<ul>
<li>In bagging, some instances get sampled several times for the same predicted while others may not be sampled at all.</li>
<li>Statistically, 63% of the training instances are sampled for each predictor while the rest 37% are not sampled. (It is not the same 63% and 37% for all the predictors!)</li>
<li>These unsampled instances (with respect to a predictor) are called <em>out-of-bag</em> instances.</li>
<li>A bagging ensemble can thus be evaluated with <em>oob</em> instances without the need for a separate validation set.</li>
<li>If there are several estimators, then each instance in the training set will likely be an <em>oob</em> instance for several estimators, so these estimators can be used to make a fair ensemble prediction for that instance.</li>
</ul>
<h3>Is it benificial to have a higher randomness in our forest? Explain why or why not.</h3>
<ul>
<li>The randomness of a Random forest can be increased by using random threshold at each split in addition to choosing random subset of features.</li>
<li>Increasing the randomness trades a higher bias for a lower variance, thus yeilding an overall better model.</li>
<li>Increasing the randomness also increases the training speed of Random Forest since it is no longer bound to use the best threshold for each feature at each node.</li>
<li>However, in general it is hard to tell in advance whether a Random Forest classifier will perform better or worse than an Extra Trees Classifier. The only way to know is to try both and compare them using cross-validation.</li>
</ul>
<h3>What is the basic idea behind AdaBoost? What is a drawback?</h3>
<ul>
<li>AdaBoost stands for <em>Adaptive Boosting</em>.</li>
<li>Generally in boosting methods, several predictors are trained sequentially, each trying to correct its predecessor.</li>
<li>AdaBoost works by making new predictors focus on the instances that were underfitted.</li>
<li>A base classifier is first created and it is used to make predictions on the training set.</li>
<li>The algorithm then increases the relative weight of the misclassified training instances.</li>
<li>A second classifier is created using the updated weights, evaluated with the training set and the weights are again updated. This process is repeated until some convergence criteria is met.</li>
<li><strong>Disadvantage</strong>: As this is a sequential learning technique, it cannot be parallelized (or only partially) since each successive predictor can be trained only after its previous predictor is trained and evaluated. As a result, this does not scale so well as bagging or pasting.</li>
</ul>
<h3>What is the idea behind stacking?</h3>
<ul>
<li>Stacking stands for <em>Stacked Generalization</em></li>
<li>In stacking, insted of using trivial functions (hard voting, mean, mode, etc...) for aggregating the predictions of all predictors, a dedicated machine learning model is trained to perfrom the aggregation.</li>
<li>The final predictor is stacked above all the other predictors belonging to the ensemble. It is also called a <em>blender</em> or a <em>meta learner</em>.</li>
<li>The final predictor takes all the predictions as inputs and makes the final prediction.</li>
</ul></p>
</div>
      <hr>

     <!-- Footer -->
     

<footer>
  <center>&copy; s4m13337 All Rights Reserved</center>
</footer>

</div>
</body>
</html>