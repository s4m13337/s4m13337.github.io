<!DOCTYPE html>
<html>
  <head>
    <title>Chapter 9: Unsupervised Learning - s4m13337</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/style.css">
  </head>
  <body>
  	<div class="container">
  		<h1 class="header">s4m13337's website</h1>

      <!-- Navigation -->
      
      
      <nav>
        <ul>
        
          <li><a href="/index.html">üè† Home</a></li>
        
          <li><a href="/about.html">‚ÑπÔ∏è About</a></li>
        
          <li><a href="/blog/index.html">üìù Blog</a></li>
        
          <li><a href="/contact.html">‚úâÔ∏è Contact</a></li>
        
        </ul>
      </nav>

      <!-- Content area -->
      <hr>
      <div class="content">
	<h1>Chapter 9: Unsupervised Learning</h1>
	<p><h2>Questions</h2>
<h3>What is the major benefit of unsupervised learning?</h3>
<ul>
<li>In ensemble learning, a group of predictors are trained.</li>
<li>The outputs from all the predictors are aggregated in order to obtain the final output.</li>
</ul>
<h3>Describe the tasks Clustering, Anomaly Detection and Density Estimation in one sentence.</h3>
<ul>
<li><em>Clustering</em><ul>
<li>The goal here is to group similar instances into <em>clusters</em>.</li>
<li>Examples: Recommender system, image segmentation, search engines.</li>
</ul>
</li>
<li><em>Anomaly detection</em><ul>
<li>The objective is to learn what normal data looks like and then use it to detect abnormal instances.</li>
<li>Examples: Identifying defective items on a production line, detecting unusual trends in time series, credit card frauds.</li>
</ul>
</li>
<li><em>Density estimation</em><ul>
<li>The goal is to estimate the <em>probability density function (PDF)</em> of the random process that generates the datasets.</li>
<li>Examples: It is used for anomaly detection; instances in low density regions are likely to be anomalies.</li>
</ul>
</li>
</ul>
<h3>In which applications is clustering used?</h3>
<p>Clustering is used in wide range of applications:</p>
<ul>
<li>Customer segmentation</li>
<li>Data analysis</li>
<li>Dimensionality reduction</li>
<li>Anomaly/Outlier detection</li>
<li>Semi-supervised learning</li>
<li>Search engines</li>
<li>Image segmentation</li>
</ul>
<h3>Explain the difference between hard and soft clustering.</h3>
<ul>
<li><em>Hard clustering</em>: Each instance is assigned to a single cluster.</li>
<li><em>Soft clustering</em>: Each instance receives a score per cluster. The score is usually the distance to the centroid of the cluster, or a similarity/affinity scored calculated using the Gaussian Radial Basis function.</li>
</ul>
<h3>Describe the K-Means algorithm in your own words.</h3>
<ul>
<li>Start by placing the centroids randomly (e.g. by picking \( k\) instances at random and using their locations as the centroid).</li>
<li>Then label the instances around the centroids and then update the centroids. Repeat this until the centroid stops moving.</li>
<li>The algorithm is guaranteed to converge in a finite number of steps.</li>
</ul>
<h3>What can happen if we get bad random initial centroids?</h3>
<p>If the centroid initialization is bad, then the clustering algorithm may not converge to the correct solution.</p>
<h3>How can we initialize the centroids better?</h3>
<ul>
<li>If you know the approximate location of the centroids, then it can be manually initialized.</li>
<li>Another solution is to run the algorithm multiple times with different random initializations and keep the best solution.</li>
<li>To identify the best solution, we use the performance metric <em>inertia</em>.</li>
</ul>
<h3>How can we measure the performance of K-Means?</h3>
<ul>
<li>The performance of K-Means is measured using the model's <em>inertia</em>.</li>
<li>The inertia is the mean squared distance between each instance and the nearest centroid.</li>
<li>The lower the inertia is, the better the model is.</li>
</ul>
<h3>Why is minimizing inertia a bad metric if we try to get the best number of clusters?</h3>
<ul>
<li>The inertia is not a good performance metric for choosing the number of clusters \( k \) because it keeps getting lower as we increase \( k \).</li>
<li>The inertia drops very fast for lower number of clusters until the elbow point beyond which it decreases very slowly.</li>
<li>Any value lower than the elbow point would be dramatic and anything higher would not help much and we might end up splitting good clusters for no reason.</li>
</ul>
<p><center><img src="/static/images/mlwr/ch_9_elbow.png" width="600"></center></p>
<h3>How can we find a good guess for k?</h3>
<ul>
<li>A precise but computationally expensive approach to select the number of clusters is to use he <em>silhouette score</em>, which is the mean <em>silhouette coefficient</em> over all the instances.</li>
<li>An instance's silhouette coefficient is given by \( \frac{(b-a)}{\text{max}(a, b)} \) where \( a \) is the mean distance to other instances in the same cluster and \( b \) is the mean distance to the instances of the next closest cluster.</li>
<li>The sihouette coefficient varies between -1 and +1. A silhouette coefficient of 1 means that the instance is deep inside the cluster; 0 means it is at the boundary and -1 means it is outside the cluster/assigned to a wrong cluster.</li>
</ul>
<h3>How can we interpret these silhouette diagrams?</h3>
<p><center><img src="/static/images/mlwr/ch_9_silhouette.png" width=600></center></p>
<ul>
<li>Each diagram contains one knife per cluster.</li>
<li>The shape's height indicates the number of instances the cluster contains and its width represents the sorted silhouette coefficients of the instances in the cluster (wider is better).</li>
<li>The dashed line indicates the mean silhouette coefficient. It also represents the silhouette score for each cluster.</li>
<li>When most of the instances in a cluster have lower coefficient than this score, then the cluster is bad.</li>
<li>In the given diagram, for \( k = 3 \) and \( k = 6 \) we get bad clusters.</li>
<li>For \( k = 4 \) the cluster at index 1 is rather big and the silhouette score is the highest.</li>
<li>When \( k = 5 \) the clusters are all equally sized. Therefore, even though the silhouette score is less than for \( k = 4 \), \( k = 5\) is the best choice to get clusters of similar sizes.</li>
</ul>
<h3>What are the limitations of K-Means?</h3>
<ul>
<li>It is necessary to run the algorithm several times to avoid sub-optimal solutions.</li>
<li>Specifying the number of clusters can be quite a hassle.</li>
<li>K-Means does not behave properly when the clusters have varying sizes, different densities or non-spherical shapes (e.g. ellipsoidal clusters).</li>
</ul>
<h3>What is the idea behind clustering for image segmentation?</h3>
<ul>
<li>Given an image, the K-Means algorithm can be used to identify clusters of similar colors.</li>
<li>Then for each color in a cluster, it looks for the mean color (e.g mean for different shades of green would be lime green).</li>
<li>Then all the pixels belonging to a cluster are replaced with the mean color. This image now comprises of the normalized colors and it can be used as a mask to segment the original image.</li>
</ul>
<h3>What is the idea behind label propagation?</h3>
<ul>
<li>Assume that we have a dataset where only a few instances are labeled.</li>
<li>Using this dataset, we can create clusters where are centroids are defined by the labeled instances.Thus each label would get its own cluster.</li>
<li>Now, the label of the instance at the centroid is propagated to all the instances in the same cluster.</li>
</ul>
<h3>What does Active Learning mean?</h3>
<ul>
<li>Active learning is when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them.</li>
<li>The most common strategy for Active learning is <em>uncertainty sampling</em>:<ul>
<li>The model is trained on athe labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances.</li>
<li>The instances for which the model is uncertain i.e. when the estimated probability is lowest, they are given the expert to labeled.</li>
<li>This process is iterated until the performance improvement stops being worth the labeling effort.</li>
</ul>
</li>
</ul>
<h3>Which instances are taken together in clusters using the DBSCAN algorithm?</h3>
<ul>
<li>For each instance, the DBSCAN algorithm counts howm amny instances are located within a small distance \( \epsilon \) from it. This region is called the instance's \( \epsilon \) neighborhood.</li>
<li>Only those instances that are located close enough are taken together.</li>
</ul>
<h3>What is a core instance in DBSCAN?</h3>
<p>If an instance has at least <code>min_samples</code> instances in its \( \epsilon \) neighborhood (including itself), then it is considered a core instance.</p></p>
</div>
      <hr>

     <!-- Footer -->
     

<footer>
  <center>&copy; s4m13337 All Rights Reserved</center>
</footer>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</div>
</body>
</html>