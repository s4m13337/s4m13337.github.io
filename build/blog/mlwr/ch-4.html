<!DOCTYPE html>
<html>
  <head>
    <title>Chapter 4: Training Models - s4m13337</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/style.css">
  </head>
  <body>
  	<div class="container">
  		<h1 class="header">s4m13337's website</h1>

      <!-- Navigation -->
      
      
      <nav>
        <ul>
        
          <li><a href="/index.html">üè† Home</a></li>
        
          <li><a href="/about.html">‚ÑπÔ∏è About</a></li>
        
          <li><a href="/blog/index.html">üìù Blog</a></li>
        
          <li><a href="/contact.html">‚úâÔ∏è Contact</a></li>
        
        </ul>
      </nav>

      <!-- Content area -->
      <hr>
      <div class="content">
	<h1>Chapter 4: Training Models</h1>
	<p><h2>Questions</h2>
<h3>What different ways are there to train a Linear Regression model? Which method provides the best results?</h3>
<ul>
<li>There are two different ways to train a linear regression model:<ul>
<li><strong>Using Closed-form equation</strong>: This method directly computes the model parameters that best fit the model to the training set.</li>
<li><strong>Gradient descent algorithm</strong>: This is a generalized iterative algorithm that gradually tweaks the model parameters to minimize the cost function over the training set.</li>
</ul>
</li>
<li>Both the methods ususally converge to the same solution. However the computational complexity of the first method is much higher datasets with large number of features and instances. The latter method has less comuptational complexity and is better suited for large datasets.</li>
<li>Also, the gradient descent algorithm can be used to train many other models in addition to linear regression models.</li>
</ul>
<h3>What is the common performance measure for regression?</h3>
<ul>
<li>The most common performance measure for regression is the <strong>Root Mean Square Error (RMSE)</strong>. \begin{equation} \text{RMSE}(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2} \end{equation}</li>
<li>In linear regression, to train a model, we need to find the parameters \( \theta \) that minimize the RMSE.</li>
<li>However, in practice, it is much simpler to minimize the <strong>Mean Square Error (MSE)</strong> than the RMSE and it leads to the same result because the value that minimizes a function also minimizes its square root. \begin{equation} \text{MSE}(X, h) = \frac{1}{m} \sum_{i=1}^m (h(x^{(i)}) - y^{(i)})^2 \end{equation}</li>
</ul>
<h3>Which is more computationally intensive for the closed form calculation, doubling the number of instances or doubling the number of features?</h3>
<ul>
<li>The computational complexity of closed for calculation grows as \( O(n^{2.4})\) to \( O(n^3) \) with respect to the number of features where as with respect to the number of instances it grows linearly (\( O(m)\)).</li>
<li>Thus doubling the number of features would increase the computation time by \( 2^{2.4} = 5.3 \) to \(2^3 = 8 \) times.</li>
</ul>
<h3>How does Polynomial Regression work?</h3>
<ul>
<li>A linear model can be used to fit nonlinear data.</li>
<li>This is done by adding powers of each features as a new feature and then train a linear model on this extended set of features. This technique is called <strong>Polynomial regression</strong>.</li>
</ul>
<h3>Summarize the steps done using Gradient Descent.</h3>
<ul>
<li>The general idea of Gradient Descent is to tweak the learning paramenters iteratively in order to minimize the cost function.</li>
<li>It starts by assigning random initial values for the parameters.</li>
<li>The local gradient of the cost function (MSE in case of regression) is computed with respect to the learning parameters.</li>
<li>The parameters are then updated by subtracting the gradient vector weighted by the learning rate, from the initial values. This is repeated until the the global minimum is obtained.</li>
</ul>
<h3>What can be a problem with setting the learning rate to high or to low?</h3>
<ul>
<li>If the learning rate is too low, the algorithm will require larger number of iterations and thus take longer time to converge.</li>
<li>If the learning rate is too high, then the algorithm will oscillate or move farther away from the optimal points.</li>
</ul>
<h3>What is a problem with Batch Gradient Descent (or better "Full Gradient Descent")?</h3>
<ul>
<li>Batch gradient descent computes the gradient over the full training set.</li>
<li>The the training set size is large, the gradient computation would become extremely slow.</li>
</ul>
<h3>What is a problem with Stochastic Gradient Descent?</h3>
<ul>
<li>Stochastic gradient descent picks up a random instance from the training set in every iteration and computes the gradient based only on that single instance.</li>
<li>Due to this stochastic nature, this algorithm is less regular than Batch Gradient Descent.</li>
<li>Instead of gradually decreasing to the global minimum, it bounces around, decreasing only on an average.</li>
<li>Over the time it gets close to the global minimum but never settles down at the minimum.</li>
</ul>
<h3>What is the intuition behind a learning schedule?</h3>
<ul>
<li>The randomness in Stochastic gradient descent helps it to escape from a local minima. But the same randomness also prevents it from settling down at the global minima.</li>
<li>To solve this, the learning rate can be reduced at every step. The algorithm starts with a high learning rate taking large steps to escape any local minima and then it gets smaller and smaller.</li>
<li>The function that determines the learning rate at every step is called the learning schedule.</li>
</ul>
<h3>What is the intuition behind Mini-batch Gradient Descent?</h3>
<ul>
<li>Mini-batch gradient descent combines the benefits of both Batch gradient descent and stochastic gradient descent.</li>
<li>At every iteration it, the algorithm randomly chooses small sets of instances called mini-batches and computes the gradients.</li>
</ul></p>
</div>
      <hr>

     <!-- Footer -->
     

<footer>
  <center>&copy; s4m13337 All Rights Reserved</center>
</footer>

</div>
</body>
</html>