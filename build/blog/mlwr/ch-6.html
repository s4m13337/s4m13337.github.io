<!DOCTYPE html>
<html>
  <head>
    <title>Chapter 6: Decision Trees - s4m13337</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/static/style.css">
  </head>
  <body>
  	<div class="container">
  		<h1 class="header">s4m13337's website</h1>

      <!-- Navigation -->
      
      
      <nav>
        <ul>
        
          <li><a href="/index.html">üè† Home</a></li>
        
          <li><a href="/about.html">‚ÑπÔ∏è About</a></li>
        
          <li><a href="/blog/index.html">üìù Blog</a></li>
        
          <li><a href="/contact.html">‚úâÔ∏è Contact</a></li>
        
        </ul>
      </nav>

      <!-- Content area -->
      <hr>
      <div class="content">
	<h1>Chapter 6: Decision Trees</h1>
	<p><h2>Questions</h2>
<h3>What are the benefits of Decision Trees?</h3>
<ul>
<li>Decision Trees require very little data preparation.</li>
<li>They do not require any feature scaling.</li>
</ul>
<h3>How does a Decision Tree predict a class?</h3>
<ul>
<li>Let's consider the example for predicting the class of Iris flowers.</li>
<li>At the root node, we check for an instance if its petal length is smaller than 2.45 cm. If this is true, then we move to the left child and classify the instance as class setosa.</li>
<li>If the petal length is greater than 2.45 cm, we move to the right child and check if the petal width is less than 1.45 cm. If this is true then the instance is classified as versicolor otherwise we classify it as virginica.</li>
<li>In essence, at every node, a decision is taken based on a feature. Based on this decision instances get classified.</li>
</ul>
<h3>How is the Gini score calculated?</h3>
<ul>
<li>The Gini score determines the degree of impurity of a node.</li>
<li>If the Gini score is 0, it means that all the instances in the node belong to the same calss.</li>
<li>Gini score is computed as \begin{equation*} G_i = 1 - \sum_{k=1}^n P_{i, k}^2 \end{equation*} where \( P_{i, k} \) is the ratio of instances belonging to a class to the total number of instances in the node.</li>
</ul>
<h3>Why are Decision Trees called "White Box" models?</h3>
<ul>
<li>Decision trees are intuitive and their decisions are easy to interpret.</li>
<li>They provide nice, simple classification rules that can even be applied manually.</li>
</ul>
<h3>How does the CART training algorithm work?</h3>
<ul>
<li>CART stands for <em>Classification And Regression Tree</em>.</li>
<li>The algorithm starts by splitting the training set into two subsets based on a single feature \( k \) and a threshold \( t_k \) (e.g. petal length \( \leq \) 2.45 cm)</li>
<li>The choise of \( k \) and \( t_k \) is based on the search for the pair \( (k, t_K) \) that produces the purest subsets (weighted by their size). The following equation describes the cost function that this algorithm tries to minimize: \begin{equation*} J(k, t_k) = \frac{m_{\text{left}}}{m}G_{\text{left}} + \frac{m_{\text{right}}}{m}G_{\text{right}} \end{equation*} where,<br />
    \( G_{\text{left/right}} \) measures the impurity of the left/right subset and,<br />
    \( m_{\text{left/rigt}} \) is the number of instances in the left/right subset.</li>
<li>Once the CART algorithm splits the training set into two, it uses the same logic to recursively split into further subsets.</li>
<li>The recursion stops once it reaches the maximum depth defined by <code>max_depth</code> hyperparameter or if it is unable to find a split that can reduce the impurity.</li>
</ul>
<h3>What is the overall prediction complexity? And why?</h3>
<ul>
<li>Making predictions requires traversing the Decision Tree from the root to a leaf.</li>
<li>Decision trees are generally approximately balanced. So traversing them requires roughly \( O(\log_2(m)) \) nodes.</li>
<li>Since each node requires checking the value of one feature, the overall prediction complexity is \( O(\log_2(m)) \) and it is independent of the number of features.</li>
<li>Thus, predictions are fast even with dealing with very large training sets.</li>
</ul>
<h3>What ist the overall training complexity? And why?</h3>
<ul>
<li>The training algorithm compares all features on all samples at each node.</li>
<li>This results in a training complexity of \( O(n \times m \log_2(m))\).</li>
</ul>
<h3>What is "Entropy"?</h3>
<ul>
<li><em>Thermodynamics</em>: Measure of molecular disorder - Entropy approaches zero when molecules are still and well ordered.</li>
<li><em>Information theory</em>: Measure of average information content of a message - Entropy is zero when messages are identical.</li>
<li><em>Machine learning</em>: Measure of impurity: A set's entropy is zero when it contains instances of only one class. \begin{equation*} H_i = \sum_{k=1}^n P_{i,k}\log_2(P_{i, k}) \mid P_{i,k} \neq 0 \end{equation*}</li>
</ul>
<h3>Why do we use the parameter max_depth?</h3>
<ul>
<li>The hyperparameter <code>max_depth</code> is used to restrict the depth of a decision tree.</li>
<li>Restricting the <code>max_depth</code> will regularize the model and thus reduce the risk of overfittng.</li>
</ul>
<h3>What is pruning?</h3>
<ul>
<li>Certain algorithms work by training the Decision Tree without any restrictions and then <em>pruning</em> (deleting) unnecessary nodes.</li>
<li>A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not statistically significant (Typically 5% p-value).</li>
</ul>
<h3>What is a limitation of Decision Trees?</h3>
<ul>
<li>Decision trees become unstable when the dataset is rotated.</li>
<li>Decision trees are also extremely sensitive to small variations in the training data.</li>
</ul>
<h3>Describe the difference between a decision tree for classification and for regression.</h3>
<ul>
<li>In case of classification, the decision tree has to predict a class whereas in the case of regression, the model has to predict a value.</li>
<li>In classification, each node is split based on the <em>gini score</em>. In the case of regression, each node is split based on the <em>MSE</em>.</li>
</ul></p>
</div>
      <hr>

     <!-- Footer -->
     

<footer>
  <center>&copy; s4m13337 All Rights Reserved</center>
</footer>

</div>
</body>
</html>